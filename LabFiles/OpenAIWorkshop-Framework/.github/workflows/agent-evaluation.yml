name: Agent Evaluation

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Runs agent quality evaluation against a deployed backend and logs
# results to an INDEPENDENT Azure AI Foundry project for trend tracking.
#
# Architecture decision: The Foundry project ("evaluate" in RG "ml")
# is intentionally separate from pipeline-managed infrastructure.
# Pipeline runs destroy-infrastructure on dev branches, so if Foundry
# were part of that infra, every run would wipe evaluation history.
# Keeping it independent enables cross-environment trend comparison
# and zero provisioning overhead per run.
#
# Auth: Uses OIDC (DefaultAzureCredential) â€” no API keys needed.
# Required RBAC roles on the GitHub Actions service principal:
#   - Azure AI User          â†’ on the Foundry hub/project
#   - Cognitive Services OpenAI User â†’ on the AI Services resource
#   - Storage Blob Data Contributor  â†’ on the Foundry-linked storage
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

on:
  workflow_call:
    inputs:
      environment:
        type: string
        required: false
        default: 'dev'
      backend_endpoint:
        type: string
        required: true
        description: 'Backend API endpoint URL'
      eval_limit:
        type: number
        required: false
        default: 5
        description: 'Max test cases to run (0 = all)'

  workflow_dispatch:
    inputs:
      environment:
        description: Target environment
        type: choice
        options: [dev, integration, prod]
        default: dev
      backend_endpoint:
        description: 'Backend API endpoint URL'
        required: true
      eval_limit:
        description: 'Max test cases (0 = all, default 5)'
        required: false
        default: '5'

jobs:
  agent-evaluation:
    name: Agent Quality Evaluation
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment || 'integration' }}
    permissions:
      contents: read
      id-token: write   # Needed for OIDC â†’ DefaultAzureCredential

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Azure OIDC Login
        uses: azure/login@v2
        with:
          client-id: ${{ vars.AZURE_CLIENT_ID }}
          tenant-id: ${{ vars.AZURE_TENANT_ID }}
          subscription-id: ${{ vars.AZURE_SUBSCRIPTION_ID }}

      - name: Install evaluation dependencies
        run: |
          pip install --upgrade pip
          # --pre is required so azure-ai-projects>=2.0.0b2 resolves to the
          # 2.x pre-release (2.0.0b3) instead of the 1.0.0 GA release.
          # The 2.x SDK is needed for azure_ai_evaluator support.
          pip install --pre \
            azure-ai-evaluation>=1.14.0 \
            azure-ai-projects>=2.0.0b2 \
            azure-identity>=1.19.0 \
            httpx>=0.28.0 \
            requests>=2.32.0 \
            python-dotenv>=1.0.0 \
            openai>=2.5.0

      - name: Verify SDK versions
        run: |
          echo "Installed SDK versions:"
          pip show azure-ai-projects openai azure-ai-evaluation 2>/dev/null | grep -E 'Name:|Version:'
          echo "---"
          python -c "from azure.ai.projects import AIProjectClient; print('AIProjectClient imported OK')"

      - name: Wait for backend to be ready
        run: |
          echo "Waiting for backend at ${{ inputs.backend_endpoint }}..."
          for i in $(seq 1 12); do
            if curl -sf "${{ inputs.backend_endpoint }}/auth/config" > /dev/null 2>&1; then
              echo "âœ… Backend is ready"
              exit 0
            fi
            echo "  Attempt $i/12 â€” retrying in 10s..."
            sleep 10
          done
          echo "::error::Backend not reachable after 2 minutes"
          exit 1

      - name: Run agent evaluation
        id: eval
        run: |
          cd agentic_ai/evaluations
          python run_agent_eval.py \
            --backend-url "${{ inputs.backend_endpoint }}" \
            --agent "ci_${GITHUB_REF_NAME}" \
            --local \
            --remote \
            --single-turn-only \
            --limit ${{ inputs.eval_limit }} \
            --ci \
            2>&1 | tee eval_output.txt

          echo "eval_completed=true" >> $GITHUB_OUTPUT
        continue-on-error: true   # Report results but don't fail the pipeline
        env:
          # AI Foundry auth â€” OIDC provides DefaultAzureCredential, no API key needed
          AZURE_AI_PROJECT_ENDPOINT: ${{ vars.AZURE_AI_PROJECT_ENDPOINT }}
          AZURE_OPENAI_ENDPOINT: ${{ vars.AZURE_OPENAI_EVAL_ENDPOINT }}
          AZURE_OPENAI_EVAL_DEPLOYMENT: ${{ vars.AZURE_OPENAI_EVAL_DEPLOYMENT }}
          # Note: Do NOT set AZURE_OPENAI_API_VERSION or OPENAI_API_VERSION here.
          # The azure-ai-projects 2.x SDK sets api-version=2025-11-15-preview
          # automatically, which is required for azure_ai_evaluator support.

      - name: Generate evaluation summary
        if: always()
        run: |
          echo "## ðŸ§ª Agent Evaluation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Setting | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Backend | \`${{ inputs.backend_endpoint }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Environment | \`${{ inputs.environment }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Test limit | ${{ inputs.eval_limit }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Branch | \`${GITHUB_REF_NAME}\` |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f agentic_ai/evaluations/eval_output.txt ]; then
            echo "### Score Summary" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            sed -n '/EVALUATION SUMMARY/,/evaluation complete/p' agentic_ai/evaluations/eval_output.txt >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "See workflow logs for details" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“Š **Full results**: View in [Azure AI Foundry](https://ai.azure.com) â†’ Project \`evaluate\` â†’ Evaluations" >> $GITHUB_STEP_SUMMARY

      - name: Upload evaluation artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: agent-evaluation-results
          path: |
            agentic_ai/evaluations/eval_results/
            agentic_ai/evaluations/eval_output.txt
          retention-days: 30
          if-no-files-found: ignore
